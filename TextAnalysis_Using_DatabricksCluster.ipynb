{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720b6477-0162-4bfc-b891-cf65f014ebf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, ArrayType, DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"TextAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ba1718-24da-4175-b065-728e049ff047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            document|          ext_labels|                  id|           rg_labels|             summary|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|i keep seeing pos...|   0,0,1,1,0,0,0,0,0|TLDR_RS_2021-03-c...|0.274461631576391...|whats the discour...|\n",
      "|so i 'm already t...|0,0,1,0,0,0,0,0,0...|TLDR_RS_2021-04-c...|0.062915374070476...|double everything...|\n",
      "|so this [ compila...|0,0,0,0,0,0,0,0,1...|TLDR_RS_2021-04-c...|0.048246581723196...|did nt you , you ...|\n",
      "|so as we all know...|     0,0,0,0,0,1,0,0|TLDR_RS_2021-04-c...|0.163933538860767...|citadel and other...|\n",
      "|researching educa...|0,0,0,0,0,0,0,0,0...|TLDR_RS_2021-02-c...|0.011458341397500...|if you want to be...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv('dbfs:/FileStore/cleaned_data.csv')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca3db49-846d-4257-9d35-88f006195e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Fill NaN values in 'document' and 'summary'\n",
    "df = df.fillna({'document': '', 'summary': ''})\n",
    "\n",
    "# VADER Sentiment Analysis\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define UDF for sentiment analysis\n",
    "def vader_sentiment_udf(text):\n",
    "    score = sia.polarity_scores(text)['compound']\n",
    "    return score\n",
    "\n",
    "# Register UDF\n",
    "vader_udf = udf(vader_sentiment_udf, DoubleType())\n",
    "\n",
    "# Apply the UDF to compute sentiment scores\n",
    "df = df.withColumn(\"document_sentiment\", vader_udf(col(\"document\")))\n",
    "df = df.withColumn(\"summary_sentiment\", vader_udf(col(\"summary\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79fa166-c175-451e-9f6d-709a4cd51f9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+-----------------+\n",
      "|            document|document_sentiment|             summary|summary_sentiment|\n",
      "+--------------------+------------------+--------------------+-----------------+\n",
      "|i keep seeing pos...|          Positive|whats the discour...|          Neutral|\n",
      "|so i 'm already t...|          Positive|double everything...|          Neutral|\n",
      "|so this [ compila...|          Negative|did nt you , you ...|         Negative|\n",
      "|so as we all know...|          Negative|citadel and other...|         Negative|\n",
      "|researching educa...|          Negative|if you want to be...|         Positive|\n",
      "+--------------------+------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interpret sentiment scores\n",
    "def interpret_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif score <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Register interpret sentiment as UDF\n",
    "interpret_udf = udf(interpret_sentiment, StringType())\n",
    "\n",
    "df = df.withColumn(\"document_sentiment\", interpret_udf(col(\"document_sentiment\")))\n",
    "df = df.withColumn(\"summary_sentiment\", interpret_udf(col(\"summary_sentiment\")))\n",
    "\n",
    "df[['document', 'document_sentiment', 'summary', 'summary_sentiment']].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "603d8a30-c776-4345-ac9a-51b4c83e799c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(text):\n",
    "    return text.split() if isinstance(text, str) else []\n",
    "\n",
    "tokenize_udf = udf(tokenize, ArrayType(StringType()))\n",
    "\n",
    "df = df.withColumn(\"document_token\", tokenize_udf(col(\"document\")))\n",
    "df = df.withColumn(\"summary_token\", tokenize_udf(col(\"summary\")))\n",
    "\n",
    "# Topic Modeling\n",
    "def perform_lda(tokens_list):\n",
    "    if tokens_list:\n",
    "        dictionary = corpora.Dictionary([tokens_list])\n",
    "        corpus = [dictionary.doc2bow(tokens_list)]\n",
    "        lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=1, random_state=42)\n",
    "        topic_words = lda_model.show_topic(0, topn=5)\n",
    "        return \" + \".join([word for word, _ in topic_words])\n",
    "    return \"No Topic\"\n",
    "\n",
    "lda_udf = udf(perform_lda, StringType())\n",
    "\n",
    "df = df.withColumn(\"document_topics\", lda_udf(col(\"document_token\")))\n",
    "df = df.withColumn(\"summary_topics\", lda_udf(col(\"summary_token\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108d1377-511a-4a41-88e2-19047d5c5876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|            document|     document_topics|             summary|      summary_topics|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|i keep seeing pos...|and + to + are + ...|whats the discour...|discourse + on + ...|\n",
      "|so i 'm already t...|. + the + to + i ...|double everything...|. + i + to + just...|\n",
      "|so this [ compila...|the + . + you + a...|did nt you , you ...|. + you + to + , ...|\n",
      "|so as we all know...|the + . + , + thi...|citadel and other...|the + with + and ...|\n",
      "|researching educa...|and + the + , + t...|if you want to be...|i + , + . + 'm + and|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['document', 'document_topics', 'summary', 'summary_topics']].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb68ab5-009a-4186-b04e-76fd22579566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Convert array columns to string columns\n",
    "df = df.withColumn(\"document_token\", concat_ws(\" \", col(\"document_token\")))\n",
    "df = df.withColumn(\"summary_token\", concat_ws(\" \", col(\"summary_token\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f585e5-baf8-4d8e-8d49-40dad916017e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Output path for the CSV file\n",
    "output_path = \"dbfs:/FileStore/text_analysed_data.csv\"\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.write.option(\"header\", \"true\").csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2260bbf3-1898-40c3-9b36-33e8e3b1d9ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TextAnalysis_Using_DatabricksCluster",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
